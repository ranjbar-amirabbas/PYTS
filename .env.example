# Persian Transcription API - Environment Configuration
# ============================================================================
# SETUP INSTRUCTIONS:
# 1. Copy this file to .env: cp .env.example .env
# 2. Adjust values as needed for your environment
# 3. The service will automatically load settings from .env
# 
# All settings have sensible defaults and are optional.
# You can run the service without creating a .env file.
# ============================================================================

# =============================================================================
# Whisper Model Configuration
# =============================================================================
# Model size affects accuracy and performance trade-offs
# Options: tiny, base, small, medium, large
# 
# Performance Characteristics:
# - tiny:   Fastest, lowest accuracy (~1GB RAM, ~32x realtime)
# - base:   Fast, low accuracy (~1GB RAM, ~16x realtime)
# - small:  Balanced (~2GB RAM, ~6x realtime)
# - medium: Good accuracy, moderate speed (~5GB RAM, ~2x realtime) [RECOMMENDED]
# - large:  Best accuracy, slower (~10GB RAM, ~1x realtime)
# 
# Note: Model is downloaded on first run and cached for future use
WHISPER_MODEL_SIZE=medium

# =============================================================================
# Concurrency Configuration
# =============================================================================
# Maximum number of concurrent transcription workers
# Adjust based on available CPU cores and memory
# Each worker processes one transcription job at a time
# 
# Recommendations:
# - For CPU-only: Set to number of CPU cores (e.g., 4 for quad-core)
# - For GPU: Can be higher (8-16) depending on GPU memory
# - Each worker needs ~1-10GB RAM depending on model size
MAX_CONCURRENT_WORKERS=4

# Maximum number of jobs that can be queued
# Jobs beyond this limit will be rejected with 503 Service Unavailable
# Higher values allow more buffering but use more memory
MAX_QUEUE_SIZE=100

# =============================================================================
# File Upload Configuration
# =============================================================================
# Maximum audio file size in megabytes
# Larger files require more memory and processing time
MAX_FILE_SIZE_MB=500

# =============================================================================
# API Configuration
# =============================================================================
# Port to expose the API on the host machine
# The container always uses port 8000 internally
# Change this if port 8000 is already in use on your host
# Example: Set to 8080 to access API at http://localhost:8080
API_PORT=8000

# API host address (usually 0.0.0.0 for Docker)
# 0.0.0.0 allows connections from any network interface
# Use 127.0.0.1 to restrict to localhost only (not recommended for Docker)
API_HOST=0.0.0.0

# =============================================================================
# Logging Configuration
# =============================================================================
# Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Use DEBUG for development, INFO for production
LOG_LEVEL=INFO

# =============================================================================
# Job Management Configuration
# =============================================================================
# Maximum age of completed jobs before cleanup (in hours)
# Completed jobs older than this will be automatically removed
JOB_CLEANUP_MAX_AGE_HOURS=24

# =============================================================================
# Streaming Configuration
# =============================================================================
# Minimum chunk size for streaming audio (in bytes)
# Smaller chunks = more frequent updates but higher overhead
# Larger chunks = less frequent updates but more efficient
# Default: 100 KB (102400 bytes)
STREAM_MIN_CHUNK_SIZE=102400

# Maximum buffer size for streaming audio (in bytes)
# Prevents memory exhaustion from large streaming sessions
# Must be >= STREAM_MIN_CHUNK_SIZE
# Default: 10 MB (10485760 bytes)
STREAM_MAX_BUFFER_SIZE=10485760

# =============================================================================
# Docker Build Configuration
# =============================================================================
# These are used during docker build, not at runtime
# Uncomment to override defaults

# DOCKER_BUILDKIT=1  # Enable BuildKit for faster builds

# =============================================================================
# USAGE EXAMPLES
# =============================================================================
# 
# Development Setup (fast iteration):
# -----------------------------------
# WHISPER_MODEL_SIZE=small
# MAX_CONCURRENT_WORKERS=2
# LOG_LEVEL=DEBUG
# 
# Production Setup (balanced):
# ----------------------------
# WHISPER_MODEL_SIZE=medium
# MAX_CONCURRENT_WORKERS=4
# MAX_FILE_SIZE_MB=500
# LOG_LEVEL=INFO
# 
# High-Accuracy Setup (best quality):
# -----------------------------------
# WHISPER_MODEL_SIZE=large
# MAX_CONCURRENT_WORKERS=2
# MAX_FILE_SIZE_MB=1000
# LOG_LEVEL=INFO
# 
# Resource-Constrained Setup (low memory):
# ----------------------------------------
# WHISPER_MODEL_SIZE=tiny
# MAX_CONCURRENT_WORKERS=1
# MAX_FILE_SIZE_MB=100
# MAX_QUEUE_SIZE=10
# 
# =============================================================================
# TROUBLESHOOTING
# =============================================================================
# 
# Out of Memory Errors:
# - Reduce WHISPER_MODEL_SIZE (try small or tiny)
# - Reduce MAX_CONCURRENT_WORKERS
# - Reduce MAX_FILE_SIZE_MB
# - Increase Docker memory limit in docker-compose.yml
# 
# Slow Transcription:
# - Use smaller model (small or tiny) for faster processing
# - Reduce MAX_CONCURRENT_WORKERS to avoid CPU contention
# - Consider GPU acceleration (requires CUDA-enabled Docker)
# 
# Port Already in Use:
# - Change API_PORT to an available port (e.g., 8080, 8888)
# 
# Model Download Issues:
# - Ensure internet connection on first run
# - Model is cached in Docker volume 'whisper-models'
# - To re-download: docker-compose down -v && docker-compose up
# 
# =============================================================================
